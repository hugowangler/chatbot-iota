{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "# Local module imports\n",
    "sys.path.insert(0, \"..\") # adds parent folder to sys path\n",
    "#sys.path.insert(0, \"../datasets\")\n",
    "\n",
    "#from datasets.data_loading import preprocess_amazon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].str.lower()\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].replace(\n",
    "        \"[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+\", \"\", regex=True\n",
    "    )  # remove emails\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].replace(\n",
    "        \"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}\", \"\", regex=True\n",
    "    )  # remove IP address\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].str.replace(\n",
    "        \"[^\\w\\s]\", \"\"\n",
    "    )  # remove special characters\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].replace(\"\\d\", \"\", regex=True)  # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row[\"Sentence\"])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words(\"english\")]\n",
    "        df_ = df_.append(\n",
    "            {\n",
    "                \"index\": row[\"index\"],\n",
    "                \"Class\": row[\"Class\"],\n",
    "                \"Sentence\": \" \".join(filtered_sent[0:]),\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../datasets/amazon_cells_labelled.txt\"\n",
    "\n",
    "# get data, pre-process and split\n",
    "data = pd.read_csv(DATASET_PATH, delimiter=\"\\t\", header=None)\n",
    "data.columns = [\"Sentence\", \"Class\"]\n",
    "data[\"index\"] = data.index  # add new column index\n",
    "columns = [\"index\", \"Class\", \"Sentence\"]\n",
    "data = preprocess_pandas(data, columns)  # pre-process\n",
    "(training_data, validation_data, training_labels, validation_labels) = train_test_split(  # split the data into training, validation, and test splits\n",
    "    data[\"Sentence\"].values.astype(\"U\"),\n",
    "    data[\"Class\"].values.astype(\"int32\"),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000,\n",
    "    max_df=0.5,\n",
    "    use_idf=True,\n",
    "    norm=\"l2\",\n",
    ")\n",
    "training_data = word_vectorizer.fit_transform(training_data)  # transform texts to sparse matrix\n",
    "training_data = training_data.todense()  # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data with labels\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "validation_dataset = TensorDataset(validation_x_tensor, validation_y_tensor)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=len(training_data), shuffle=True)\n",
    "validationloader = DataLoader(validation_dataset, batch_size=len(validation_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 [1/1] - Loss: 0.6932216286659241"
     ]
    }
   ],
   "source": [
    "network = nn.Sequential(\n",
    "    nn.Linear(vocab_size, 100),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(100, 2)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_nr, (data, labels) in enumerate(trainloader):\n",
    "        prediction = network(data)\n",
    "        \n",
    "        loss = loss_function(prediction, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Print the epoch, batch, and loss\n",
    "        print(\n",
    "            f'\\rEpoch {epoch+1} [{batch_nr+1}/{len(trainloader)}] - Loss: {loss}',\n",
    "            end=''\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the network is 78.0%.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # For each batch of testing data (since the dataset is too large to run all data through the network at once)\n",
    "    for batch_nr, (data, labels) in enumerate(validationloader):\n",
    "        prediction = network(data)\n",
    "        \n",
    "        # Get the predicted category from each prediction\n",
    "        prediction = list(pred.argmax() for pred in prediction)\n",
    "        \n",
    "        # Sum all the predictions that were the same as the label\n",
    "        correct += np.equal(prediction, labels).sum().item()\n",
    "        total += len(labels)\n",
    "    \n",
    "    print(f'The accuracy of the network is {str(100*correct/total)[:4]}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print a sentence and predict it's category\n",
    "sentence_index = 0\n",
    "#prediction = torch.argmax(network(torch.unsqueeze(bow_embedder(ag_train[sentence_index][1], len(vocab)), dim=0))).item()\n",
    "#print(\n",
    "#    f'The network predicted that \\n\"{tensor2text(ag_train[sentence_index][1])}\"\\n should be in the category {ag_labels[prediction]}'\n",
    "#)\n",
    "\n",
    "sentence = \"this product sucks\"\n",
    "sentence_label = 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
